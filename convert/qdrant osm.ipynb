{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: qdrant-client[fastembed] in /opt/conda/lib/python3.10/site-packages (1.6.9)\n",
      "Requirement already satisfied: polars in /opt/conda/lib/python3.10/site-packages (0.19.14)\n",
      "Requirement already satisfied: folium in /opt/conda/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.13.1+cu116)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.14.1+cu116)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from qdrant-client[fastembed]) (2.8.2)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /opt/conda/lib/python3.10/site-packages (from qdrant-client[fastembed]) (1.59.3)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /opt/conda/lib/python3.10/site-packages (from qdrant-client[fastembed]) (1.59.3)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in /opt/conda/lib/python3.10/site-packages (from qdrant-client[fastembed]) (1.26.15)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /opt/conda/lib/python3.10/site-packages (from qdrant-client[fastembed]) (2.5.1)\n",
      "Requirement already satisfied: httpx[http2]>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from qdrant-client[fastembed]) (0.25.1)\n",
      "Collecting fastembed==0.1.1\n",
      "  Downloading fastembed-0.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting onnxruntime<2.0,>=1.15\n",
      "  Downloading onnxruntime-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests<3.0,>=2.31\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.14,>=0.13\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting onnx<2.0,>=1.11\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.9 in /opt/conda/lib/python3.10/site-packages (from folium) (3.1.2)\n",
      "Requirement already satisfied: branca>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from folium) (0.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant-client[fastembed]) (67.6.1)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /opt/conda/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant-client[fastembed]) (4.25.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client[fastembed]) (1.3.0)\n",
      "Requirement already satisfied: httpcore in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client[fastembed]) (1.0.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client[fastembed]) (2022.12.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client[fastembed]) (3.4)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client[fastembed]) (3.6.2)\n",
      "Requirement already satisfied: h2<5,>=3 in /opt/conda/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant-client[fastembed]) (4.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.9->folium) (2.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant-client[fastembed]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant-client[fastembed]) (2.14.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.31->fastembed==0.1.1->qdrant-client[fastembed]) (3.1.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /opt/conda/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client[fastembed]) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /opt/conda/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client[fastembed]) (6.0.1)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2.0,>=1.15->fastembed==0.1.1->qdrant-client[fastembed]) (23.3.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2.0,>=1.15->fastembed==0.1.1->qdrant-client[fastembed]) (1.11.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore->httpx[http2]>=0.14.0->qdrant-client[fastembed]) (0.14.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime<2.0,>=1.15->fastembed==0.1.1->qdrant-client[fastembed]) (1.3.0)\n",
      "Installing collected packages: tokenizers, requests, onnx, humanfriendly, coloredlogs, transformers, onnxruntime, fastembed\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed coloredlogs-15.0.1 fastembed-0.1.1 humanfriendly-10.0 onnx-1.15.0 onnxruntime-1.16.2 requests-2.31.0 tokenizers-0.13.3 transformers-4.33.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers qdrant-client[fastembed] polars folium transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Victor\\OneDrive - Université de Namur\\Cours\\Master1\\Data analytics\\projet Quokka\\convert\\qdrant osm.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m qdrant_client\u001b[39m.\u001b[39;49mset_model(\u001b[39m\"\u001b[39;49m\u001b[39msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/qdrant_client/qdrant_fastembed.py:50\u001b[0m, in \u001b[0;36mQdrantFastembedMixin.set_model\u001b[0;34m(self, embedding_model_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_model\u001b[39m(\u001b[39mself\u001b[39m, embedding_model_name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    Set embedding model to use for encoding documents and queries.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m        None\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_or_init_model(model_name\u001b[39m=\u001b[39;49membedding_model_name)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model_name \u001b[39m=\u001b[39m embedding_model_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/qdrant_client/qdrant_fastembed.py:87\u001b[0m, in \u001b[0;36mQdrantFastembedMixin._get_or_init_model\u001b[0;34m(cls, model_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported embedding model: \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m. Supported models: \u001b[39m\u001b[39m{\u001b[39;00mSUPPORTED_EMBEDDING_MODELS\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_import_fastembed()\n\u001b[0;32m---> 87\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39membedding_models[model_name] \u001b[39m=\u001b[39m DefaultEmbedding(model_name\u001b[39m=\u001b[39;49mmodel_name)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39membedding_models[model_name]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "qdrant_client.set_model(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.recreate_collection(\n",
    "    collection_name=\"pois\",\n",
    "    vectors_config=qdrant_client.get_fastembed_vector_params(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(row):\n",
    "    return [\n",
    "        \"category: \" + row[\"category\"],\n",
    "        \"sub category: \" + row[\"sub_category\"],\n",
    "        \"name: \" + row[\"name\"],\n",
    "        \"description: \" + row[\"description\"] if row[\"description\"] else \"\",\n",
    "        \"brand: \" + row[\"brand\"] if row[\"brand\"] else \"\",\n",
    "        \"alt_name: \" + row[\"alt_name\"] if row[\"alt_name\"] else \"\",\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 82)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>lat</th><th>lon</th><th>addr:city</th><th>addr:country</th><th>addr:housenumber</th><th>addr:postcode</th><th>addr:street</th><th>brand</th><th>brand:wikidata</th><th>brand:wikipedia</th><th>cuisine</th><th>description</th><th>drive_through</th><th>name</th><th>note</th><th>operator</th><th>takeaway</th><th>wheelchair</th><th>bicycle</th><th>information</th><th>access</th><th>wikidata</th><th>wikimedia_commons</th><th>wikipedia</th><th>website</th><th>toilets:wheelchair</th><th>alt_name</th><th>check_date</th><th>opening_hours</th><th>changing_table</th><th>contact:phone</th><th>contact:website</th><th>delivery</th><th>diet:vegetarian</th><th>indoor_seating</th><th>internet_access</th><th>&hellip;</th><th>phone</th><th>heritage</th><th>heritage:operator</th><th>image</th><th>man_made</th><th>level</th><th>operator:wikidata</th><th>wifi</th><th>old_name</th><th>seamark:harbour:category</th><th>seamark:name</th><th>seamark:type</th><th>harbour</th><th>highway</th><th>name:fr</th><th>direction</th><th>capacity</th><th>reservation</th><th>smoking</th><th>toilets:access</th><th>backrest</th><th>colour</th><th>material</th><th>seats</th><th>delivery:covid19</th><th>opening_hours:covid19</th><th>takeaway:covid19</th><th>description:covid19</th><th>payment:cash</th><th>payment:maestro</th><th>payment:mastercard</th><th>contact:mobile</th><th>url</th><th>contact:email</th><th>addr:suburb</th><th>sub_category</th><th>category</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>&hellip;</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>23656136</td><td>49.858471</td><td>6.3649698</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Waldsportplatz…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;fitness_statio…</td><td>&quot;leisure&quot;</td></tr><tr><td>26860223</td><td>49.743478</td><td>6.0898523</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Restaurant Cam…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;restaurant&quot;</td><td>&quot;amenity&quot;</td></tr><tr><td>30432808</td><td>50.856197</td><td>5.8256972</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Steenkolenmijn…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;attraction&quot;</td><td>&quot;tourism&quot;</td></tr><tr><td>31425173</td><td>50.844029</td><td>5.6890789</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Nieuwenhofpoor…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Q19630871&quot;</td><td>&quot;Category:Nieuw…</td><td>&quot;nl:Nieuwenhofp…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;city_gate&quot;</td><td>&quot;historic&quot;</td></tr><tr><td>31575884</td><td>50.844551</td><td>5.6902818</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Universiteitsb…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Q15734302&quot;</td><td>null</td><td>&quot;nl:Universitei…</td><td>&quot;https://librar…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;library&quot;</td><td>&quot;amenity&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 82)\n",
       "┌──────────┬───────────┬───────────┬───────────┬───┬────────────┬───────────┬───────────┬──────────┐\n",
       "│ id       ┆ lat       ┆ lon       ┆ addr:city ┆ … ┆ contact:em ┆ addr:subu ┆ sub_categ ┆ category │\n",
       "│ ---      ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ail        ┆ rb        ┆ ory       ┆ ---      │\n",
       "│ i64      ┆ f64       ┆ f64       ┆ str       ┆   ┆ ---        ┆ ---       ┆ ---       ┆ str      │\n",
       "│          ┆           ┆           ┆           ┆   ┆ str        ┆ str       ┆ str       ┆          │\n",
       "╞══════════╪═══════════╪═══════════╪═══════════╪═══╪════════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 23656136 ┆ 49.858471 ┆ 6.3649698 ┆ null      ┆ … ┆ null       ┆ null      ┆ fitness_s ┆ leisure  │\n",
       "│          ┆           ┆           ┆           ┆   ┆            ┆           ┆ tation    ┆          │\n",
       "│ 26860223 ┆ 49.743478 ┆ 6.0898523 ┆ null      ┆ … ┆ null       ┆ null      ┆ restauran ┆ amenity  │\n",
       "│          ┆           ┆           ┆           ┆   ┆            ┆           ┆ t         ┆          │\n",
       "│ 30432808 ┆ 50.856197 ┆ 5.8256972 ┆ null      ┆ … ┆ null       ┆ null      ┆ attractio ┆ tourism  │\n",
       "│          ┆           ┆           ┆           ┆   ┆            ┆           ┆ n         ┆          │\n",
       "│ 31425173 ┆ 50.844029 ┆ 5.6890789 ┆ null      ┆ … ┆ null       ┆ null      ┆ city_gate ┆ historic │\n",
       "│ 31575884 ┆ 50.844551 ┆ 5.6902818 ┆ null      ┆ … ┆ null       ┆ null      ┆ library   ┆ amenity  │\n",
       "└──────────┴───────────┴───────────┴───────────┴───┴────────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geo = pl.read_parquet(\"../data/transformed/poi_clean_category_geo.parquet\")\n",
    "df = df_geo.drop([\"type\", \"geometry\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63330/63330 [29:38<00:00, 35.61it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.add(\n",
    "    collection_name=\"pois\",\n",
    "    documents=[convert(row) for row in df.to_dicts()],\n",
    "    metadata=[row for row in df.to_dicts()],\n",
    "    ids=tqdm(range(df.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_loc(row):\n",
    "    row[\"location\"] = {\"lat\": row[\"lat\"], \"lon\": row[\"lon\"]}\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSearcher:\n",
    "    def __init__(self, collection_name):\n",
    "        self.collection_name = collection_name\n",
    "        # Initialize encoder model\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "        # initialize Qdrant client\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.qdrant_client.set_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    def search(self, pos, neg, around):\n",
    "        # Use `vector` for search for closest vectors in the collection\n",
    "        search_result = self.qdrant_client.recommend(\n",
    "            collection_name=self.collection_name,\n",
    "            positive=pos,\n",
    "            negative=neg,\n",
    "            strategy=models.RecommendStrategy.AVERAGE_VECTOR,\n",
    "            query_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"location\",\n",
    "                        geo_radius=models.GeoRadius(\n",
    "                            center=models.GeoPoint(\n",
    "                                lon=around[1],\n",
    "                                lat=around[0],\n",
    "                            ),\n",
    "                            radius=1000.0,\n",
    "                        ),\n",
    "                    ),\n",
    "                ],\n",
    "                # must_not=[\n",
    "                #     models.FieldCondition(\n",
    "                #         key=\"category\",\n",
    "                #         match=models.MatchValue(\n",
    "                #             value=\"amenity\",\n",
    "                #         ),\n",
    "                #     )\n",
    "                # ],\n",
    "            ),\n",
    "            limit=5,  # 5 the most closest results is enough\n",
    "        )\n",
    "        # `search_result` contains found vector ids with similarity scores along with the stored payload\n",
    "        # In this function you are interested in payload only\n",
    "        payloads = [hit.payload for hit in search_result]\n",
    "        return payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'get_added_tokens_decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Victor\\OneDrive - Université de Namur\\Cours\\Master1\\Data analytics\\projet Quokka\\convert\\qdrant osm.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m neural_searcher \u001b[39m=\u001b[39m NeuralSearcher(collection_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpois\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Victor\\OneDrive - Université de Namur\\Cours\\Master1\\Data analytics\\projet Quokka\\convert\\qdrant osm.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollection_name \u001b[39m=\u001b[39m collection_name\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Initialize encoder model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m\"\u001b[39;49m\u001b[39mall-MiniLM-L6-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# initialize Qdrant client\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Victor/OneDrive%20-%20Universit%C3%A9%20de%20Namur/Cours/Master1/Data%20analytics/projet%20Quokka/convert/qdrant%20osm.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqdrant_client \u001b[39m=\u001b[39m qdrant_client\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[0;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[0;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[0;32m    136\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[1;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39minput_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:31\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     28\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m---> 31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[0;32m     33\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m max_seq_length \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:768\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    766\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    767\u001b[0m         )\n\u001b[1;32m--> 768\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    770\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2022\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2024\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2025\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2026\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2027\u001b[0m     init_configuration,\n\u001b[0;32m   2028\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   2029\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   2030\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2031\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2032\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2033\u001b[0m     _is_local\u001b[39m=\u001b[39mis_local,\n\u001b[0;32m   2034\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2035\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2254\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2256\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs)\n\u001b[0;32m   2257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   2258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   2259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert_fast.py:221\u001b[0m, in \u001b[0;36mBertTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, do_lower_case, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    208\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    220\u001b[0m ):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    222\u001b[0m         vocab_file,\n\u001b[0;32m    223\u001b[0m         tokenizer_file\u001b[39m=\u001b[39mtokenizer_file,\n\u001b[0;32m    224\u001b[0m         do_lower_case\u001b[39m=\u001b[39mdo_lower_case,\n\u001b[0;32m    225\u001b[0m         unk_token\u001b[39m=\u001b[39munk_token,\n\u001b[0;32m    226\u001b[0m         sep_token\u001b[39m=\u001b[39msep_token,\n\u001b[0;32m    227\u001b[0m         pad_token\u001b[39m=\u001b[39mpad_token,\n\u001b[0;32m    228\u001b[0m         cls_token\u001b[39m=\u001b[39mcls_token,\n\u001b[0;32m    229\u001b[0m         mask_token\u001b[39m=\u001b[39mmask_token,\n\u001b[0;32m    230\u001b[0m         tokenize_chinese_chars\u001b[39m=\u001b[39mtokenize_chinese_chars,\n\u001b[0;32m    231\u001b[0m         strip_accents\u001b[39m=\u001b[39mstrip_accents,\n\u001b[0;32m    232\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    233\u001b[0m     )\n\u001b[0;32m    235\u001b[0m     normalizer_state \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend_tokenizer\u001b[39m.\u001b[39mnormalizer\u001b[39m.\u001b[39m__getstate__())\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    237\u001b[0m         normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m\"\u001b[39m, do_lower_case) \u001b[39m!=\u001b[39m do_lower_case\n\u001b[0;32m    238\u001b[0m         \u001b[39mor\u001b[39;00m normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m\"\u001b[39m, strip_accents) \u001b[39m!=\u001b[39m strip_accents\n\u001b[0;32m    239\u001b[0m         \u001b[39mor\u001b[39;00m normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhandle_chinese_chars\u001b[39m\u001b[39m\"\u001b[39m, tokenize_chinese_chars) \u001b[39m!=\u001b[39m tokenize_chinese_chars\n\u001b[0;32m    240\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:167\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39m# The following logic will be replace with a single add_tokens once a fix is pushed to tokenizers\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m# allows converting a slow -> fast, non-legacy: if the `tokenizer.json` does not have all the added tokens\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m# uses the information stored in `added_tokens_decoder`.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m# this is costly for fast tokenizers as we re-compute the regex again. But not all tokens are added tokens\u001b[39;00m\n\u001b[0;32m    162\u001b[0m tokens_to_add \u001b[39m=\u001b[39m [\n\u001b[0;32m    163\u001b[0m     token\n\u001b[0;32m    164\u001b[0m     \u001b[39mfor\u001b[39;00m index, token \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(added_tokens_decoder\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m])\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_decoder\n\u001b[0;32m    166\u001b[0m ]\n\u001b[1;32m--> 167\u001b[0m encoder \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madded_tokens_encoder\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m [\u001b[39mstr\u001b[39m(token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens_to_add]\n\u001b[0;32m    168\u001b[0m \u001b[39m# if some of the special tokens are strings, we check if we don't already have a token\u001b[39;00m\n\u001b[0;32m    169\u001b[0m tokens_to_add \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[0;32m    170\u001b[0m     token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens_extended \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoder \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tokens_to_add\n\u001b[0;32m    171\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:226\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.added_tokens_encoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madded_tokens_encoder\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m    222\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m    optimisation in `self._added_tokens_encoder` for the slow tokenizers.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m {k\u001b[39m.\u001b[39mcontent: v \u001b[39mfor\u001b[39;00m v, k \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madded_tokens_decoder\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m0\u001b[39m])}\n",
      "File \u001b[1;32mc:\\Users\\Victor\\miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:236\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.added_tokens_decoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madded_tokens_decoder\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mint\u001b[39m, AddedToken]:\n\u001b[0;32m    230\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m    Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39m        `Dict[str, int]`: The added tokens.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mget_added_tokens_decoder()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'get_added_tokens_decoder'"
     ]
    }
   ],
   "source": [
    "neural_searcher = NeuralSearcher(collection_name=\"pois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>name</th><th>category</th><th>sub_category</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>409005527</td><td>&quot;Le Gros Vélo&quot;</td><td>&quot;amenity&quot;</td><td>&quot;cafe&quot;</td></tr><tr><td>488390305</td><td>&quot;La croix St-Cl…</td><td>&quot;historic&quot;</td><td>&quot;wayside_cross&quot;</td></tr><tr><td>490992842</td><td>&quot;La Roche qui T…</td><td>&quot;tourism&quot;</td><td>&quot;attraction&quot;</td></tr><tr><td>491571270</td><td>&quot;Château du Lav…</td><td>&quot;historic&quot;</td><td>&quot;castle&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌───────────┬─────────────────────┬──────────┬───────────────┐\n",
       "│ id        ┆ name                ┆ category ┆ sub_category  │\n",
       "│ ---       ┆ ---                 ┆ ---      ┆ ---           │\n",
       "│ i64       ┆ str                 ┆ str      ┆ str           │\n",
       "╞═══════════╪═════════════════════╪══════════╪═══════════════╡\n",
       "│ 409005527 ┆ Le Gros Vélo        ┆ amenity  ┆ cafe          │\n",
       "│ 488390305 ┆ La croix St-Claude  ┆ historic ┆ wayside_cross │\n",
       "│ 490992842 ┆ La Roche qui Tourne ┆ tourism  ┆ attraction    │\n",
       "│ 491571270 ┆ Château du Laval    ┆ historic ┆ castle        │\n",
       "└───────────┴─────────────────────┴──────────┴───────────────┘"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [409005527, 488390305, 491571270, 490992842]\n",
    "df.select([\"id\", \"name\", \"category\", \"sub_category\"]).filter(pl.col(\"id\").is_in(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>name</th><th>category</th><th>sub_category</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>494911196</td><td>&quot;FrietJess&quot;</td><td>&quot;amenity&quot;</td><td>&quot;fast_food&quot;</td></tr><tr><td>494911197</td><td>&quot;Pizza Plaza&quot;</td><td>&quot;amenity&quot;</td><td>&quot;restaurant&quot;</td></tr><tr><td>498293266</td><td>&quot;Frit city juni…</td><td>&quot;amenity&quot;</td><td>&quot;fast_food&quot;</td></tr><tr><td>516617417</td><td>&quot;Pizza Hut&quot;</td><td>&quot;amenity&quot;</td><td>&quot;restaurant&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 4)\n",
       "┌───────────┬──────────────────┬──────────┬──────────────┐\n",
       "│ id        ┆ name             ┆ category ┆ sub_category │\n",
       "│ ---       ┆ ---              ┆ ---      ┆ ---          │\n",
       "│ i64       ┆ str              ┆ str      ┆ str          │\n",
       "╞═══════════╪══════════════════╪══════════╪══════════════╡\n",
       "│ 494911196 ┆ FrietJess        ┆ amenity  ┆ fast_food    │\n",
       "│ 494911197 ┆ Pizza Plaza      ┆ amenity  ┆ restaurant   │\n",
       "│ 498293266 ┆ Frit city junior ┆ amenity  ┆ fast_food    │\n",
       "│ 516617417 ┆ Pizza Hut        ┆ amenity  ┆ restaurant   │\n",
       "└───────────┴──────────────────┴──────────┴──────────────┘"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = [494911197, 498293266, 494911196, 516617417]\n",
    "df.select([\"id\", \"name\", \"category\", \"sub_category\"]).filter(pl.col(\"id\").is_in(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = neural_searcher.search(\n",
    "    pos=pos,\n",
    "    neg=neg,\n",
    "    around=[50.467388, 4.871985],\n",
    ")\n",
    "[\n",
    "    [\n",
    "        item[\"id\"],\n",
    "        item[\"name\"],\n",
    "        item[\"lat\"],\n",
    "        item[\"lon\"],\n",
    "        item[\"category\"],\n",
    "        item[\"sub_category\"],\n",
    "    ]\n",
    "    for item in items\n",
    "]\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_d043ca07b398482600c3b11a71c7cf1d {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_d043ca07b398482600c3b11a71c7cf1d&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_d043ca07b398482600c3b11a71c7cf1d = L.map(\n",
       "                &quot;map_d043ca07b398482600c3b11a71c7cf1d&quot;,\n",
       "                {\n",
       "                    center: [50.4672164, 4.8592783],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 15,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_ce2cc06ede491dc6ad7a3c152a5ef1fa = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "                tile_layer_ce2cc06ede491dc6ad7a3c152a5ef1fa.addTo(map_d043ca07b398482600c3b11a71c7cf1d);\n",
       "    \n",
       "            var marker_29dc4f1cddc145eb4c0b9e3986a93380 = L.marker(\n",
       "                [50.4672164, 4.8592783],\n",
       "                {}\n",
       "            ).addTo(map_d043ca07b398482600c3b11a71c7cf1d);\n",
       "        \n",
       "    \n",
       "        var popup_c6d148f05c3df0d0b355ec4d56ef5c2a = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_8e256414eb32d0cae4233d3882239734 = $(`&lt;div id=&quot;html_8e256414eb32d0cae4233d3882239734&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;10922394383 Pasta Liza&lt;/div&gt;`)[0];\n",
       "                popup_c6d148f05c3df0d0b355ec4d56ef5c2a.setContent(html_8e256414eb32d0cae4233d3882239734);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_29dc4f1cddc145eb4c0b9e3986a93380.bindPopup(popup_c6d148f05c3df0d0b355ec4d56ef5c2a)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_f9bc0ac7cdab003d76aa186b49745bf7 = L.marker(\n",
       "                [50.4702701, 4.8649146],\n",
       "                {}\n",
       "            ).addTo(map_d043ca07b398482600c3b11a71c7cf1d);\n",
       "        \n",
       "    \n",
       "        var popup_d892823a0eae29d7004d82b5a05e99c1 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_be9362af5be37f71ada5fbc738d0ff49 = $(`&lt;div id=&quot;html_be9362af5be37f71ada5fbc738d0ff49&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;9789606955 Balkan grill&lt;/div&gt;`)[0];\n",
       "                popup_d892823a0eae29d7004d82b5a05e99c1.setContent(html_be9362af5be37f71ada5fbc738d0ff49);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_f9bc0ac7cdab003d76aa186b49745bf7.bindPopup(popup_d892823a0eae29d7004d82b5a05e99c1)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_a9122ab391cb86a2ac61f7d625ec3f96 = L.marker(\n",
       "                [50.4632865, 4.8646586],\n",
       "                {}\n",
       "            ).addTo(map_d043ca07b398482600c3b11a71c7cf1d);\n",
       "        \n",
       "    \n",
       "        var popup_8b82e77049d7344a354f9b19df4ed223 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_6e2487d3f5fb2bdf32f31ee8ebf86aa7 = $(`&lt;div id=&quot;html_6e2487d3f5fb2bdf32f31ee8ebf86aa7&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;1812473011 Jack a dit&lt;/div&gt;`)[0];\n",
       "                popup_8b82e77049d7344a354f9b19df4ed223.setContent(html_6e2487d3f5fb2bdf32f31ee8ebf86aa7);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_a9122ab391cb86a2ac61f7d625ec3f96.bindPopup(popup_8b82e77049d7344a354f9b19df4ed223)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_e1bf1a80dc601363e4b13df2f2dd827a = L.marker(\n",
       "                [50.4660624, 4.8649878],\n",
       "                {}\n",
       "            ).addTo(map_d043ca07b398482600c3b11a71c7cf1d);\n",
       "        \n",
       "    \n",
       "        var popup_c6f48beab6997a5f1c16ac8638bd775b = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_ec7c1eff4fbebef273ea5fd44d1a64fc = $(`&lt;div id=&quot;html_ec7c1eff4fbebef273ea5fd44d1a64fc&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;1187583605 Dell&#x27;I Pasta&lt;/div&gt;`)[0];\n",
       "                popup_c6f48beab6997a5f1c16ac8638bd775b.setContent(html_ec7c1eff4fbebef273ea5fd44d1a64fc);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_e1bf1a80dc601363e4b13df2f2dd827a.bindPopup(popup_c6f48beab6997a5f1c16ac8638bd775b)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_7d7417a9459de1b0aabaf57efec02175 = L.marker(\n",
       "                [50.4681464, 4.8656906],\n",
       "                {}\n",
       "            ).addTo(map_d043ca07b398482600c3b11a71c7cf1d);\n",
       "        \n",
       "    \n",
       "        var popup_8129fef53d9dd812d8269915e1d6da9e = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_1dd057fe42a80971eda055281ef56c06 = $(`&lt;div id=&quot;html_1dd057fe42a80971eda055281ef56c06&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;8026432773 FNTDR Aux déportés et réfractaires&lt;/div&gt;`)[0];\n",
       "                popup_8129fef53d9dd812d8269915e1d6da9e.setContent(html_1dd057fe42a80971eda055281ef56c06);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_7d7417a9459de1b0aabaf57efec02175.bindPopup(popup_8129fef53d9dd812d8269915e1d6da9e)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f5f98e03b20>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "# create a map centered on the item's location\n",
    "m = folium.Map(location=[items[0][\"lat\"], items[0][\"lon\"]], zoom_start=15)\n",
    "\n",
    "# add a marker for the item's location\n",
    "for item in items:\n",
    "    folium.Marker(\n",
    "        location=[item[\"lat\"], item[\"lon\"]], popup=str(item[\"id\"]) + \" \" + item[\"name\"]\n",
    "    ).add_to(m)\n",
    "\n",
    "# display the map\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
